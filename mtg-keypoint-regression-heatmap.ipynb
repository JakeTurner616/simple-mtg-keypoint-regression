{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11439130,"sourceType":"datasetVersion","datasetId":7165470}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Install depends:","metadata":{}},{"cell_type":"code","source":"# Install TensorFlow (Kaggle default images usually have it pre-installed, but just in case)\n!pip install -q tensorflow tqdm opencv-python-headless matplotlib tensorflow-addons\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train the TF model to predict mtg corner locations using the synthetic dataset:","metadata":{}},{"cell_type":"code","source":"import os\nimport json\nimport cv2\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, optimizers, callbacks, regularizers\nfrom tensorflow.keras.applications import MobileNetV2\n\n# === Config ===\nINPUT_DIR        = \"/kaggle/input/mtg-keypoint-regression-dataset-5k\"\nANNOTATION_FILE  = os.path.join(INPUT_DIR, \"dataset\", \"dataset\", \"annotations.json\")\nIMAGE_DIR        = os.path.join(INPUT_DIR, \"dataset\", \"dataset\")\nOUTPUT_DIR       = \"/kaggle/working\"\nVISUALIZE_DIR    = os.path.join(OUTPUT_DIR, \"training_visualizations\")\nMODEL_PATH       = os.path.join(OUTPUT_DIR, \"mtg_keypoint_tfjs_safe_model\")\n\nIMAGE_SIZE       = 224\nHEATMAP_SIZE     = 56\nSIGMA_PIXELS     = 1.5\nBATCH_SIZE       = 16\nEPOCHS           = 35\nORIGINAL_DIM     = 1024\n\nos.makedirs(VISUALIZE_DIR, exist_ok=True)\n\n# === Load and validate annotations ===\nwith open(ANNOTATION_FILE, \"r\") as f:\n    raw = json.load(f)\n\ndef valid(a):\n    pts = a.get(\"corners\", [])\n    return (isinstance(pts, list) and len(pts) == 4 and\n            all(isinstance(x, (int,float)) and isinstance(y, (int,float)) and\n                0 <= x <= ORIGINAL_DIM and 0 <= y <= ORIGINAL_DIM\n                for x,y in pts))\n\nannotations = [a for a in raw if valid(a)]\nsplit = int(0.9 * len(annotations))\ntrain_anns, val_anns = annotations[:split], annotations[split:]\n\n# === Precompute grid for heatmaps ===\nxs = np.arange(HEATMAP_SIZE)\nys = np.arange(HEATMAP_SIZE)\ngrid_x, grid_y = np.meshgrid(xs, ys)\n\ndef make_heatmaps(corners):\n    hm = np.zeros((HEATMAP_SIZE, HEATMAP_SIZE, 4), dtype=np.float32)\n    for i, (x_o, y_o) in enumerate(corners):\n        x = (x_o / ORIGINAL_DIM) * (HEATMAP_SIZE - 1)\n        y = (y_o / ORIGINAL_DIM) * (HEATMAP_SIZE - 1)\n        d2 = (grid_x - x)**2 + (grid_y - y)**2\n        hm[..., i] = np.exp(-d2 / (2 * SIGMA_PIXELS**2))\n    return hm\n\ndef data_gen(anns):\n    while True:\n        imgs = np.zeros((BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3), dtype=np.float32)\n        hmts = np.zeros((BATCH_SIZE, HEATMAP_SIZE, HEATMAP_SIZE, 4), dtype=np.float32)\n        coords = np.zeros((BATCH_SIZE, 4, 2), dtype=np.float32)\n        for i in range(BATCH_SIZE):\n            a = np.random.choice(anns)\n            img = cv2.imread(os.path.join(IMAGE_DIR, a[\"filename\"]))\n            img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE)).astype(np.float32) / 255.0\n            hm = make_heatmaps(a[\"corners\"])\n            imgs[i]   = img\n            hmts[i]   = hm\n            coords[i] = np.array(a[\"corners\"], dtype=np.float32) / ORIGINAL_DIM\n        yield imgs, (hmts, coords)\n\ntrain_ds = tf.data.Dataset.from_generator(\n    lambda: data_gen(train_anns),\n    output_signature=(\n        tf.TensorSpec((BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3), tf.float32),\n        (tf.TensorSpec((BATCH_SIZE, HEATMAP_SIZE, HEATMAP_SIZE, 4), tf.float32),\n         tf.TensorSpec((BATCH_SIZE, 4, 2), tf.float32))\n    )\n).prefetch(tf.data.AUTOTUNE)\n\nval_ds = tf.data.Dataset.from_generator(\n    lambda: data_gen(val_anns),\n    output_signature=(\n        tf.TensorSpec((BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, 3), tf.float32),\n        (tf.TensorSpec((BATCH_SIZE, HEATMAP_SIZE, HEATMAP_SIZE, 4), tf.float32),\n         tf.TensorSpec((BATCH_SIZE, 4, 2), tf.float32))\n    )\n).prefetch(tf.data.AUTOTUNE)\n\n@tf.keras.utils.register_keras_serializable()\nclass SoftArgmax(layers.Layer):\n    def __init__(self, H, W, **kw):\n        super().__init__(**kw)\n        xs = tf.linspace(0.0, 1.0, W)\n        ys = tf.linspace(0.0, 1.0, H)\n        gx, gy = tf.meshgrid(xs, ys)\n        self.grid_x = tf.reshape(gx, [-1])\n        self.grid_y = tf.reshape(gy, [-1])\n        self.H, self.W = H, W\n\n    def call(self, heatmaps):\n        B = tf.shape(heatmaps)[0]\n        flat = tf.reshape(heatmaps, [B, self.H*self.W, 4])\n        probs = tf.nn.softmax(flat, axis=1)\n        exp_x = tf.reduce_sum(probs * self.grid_x[None,:,None], axis=1)\n        exp_y = tf.reduce_sum(probs * self.grid_y[None,:,None], axis=1)\n        return tf.stack([exp_x, exp_y], axis=-1)\n\ndef build_model():\n    inp = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name=\"input_image\")\n    base = MobileNetV2(include_top=False, input_tensor=inp, weights='imagenet')\n    x = base.output\n    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same',\n                               activation=layers.ReLU(max_value=6.0),\n                               use_bias=False,\n                               kernel_regularizer=regularizers.l2(1e-5))(x)\n    x = layers.SeparableConv2D(96, 3, padding='same',\n                               activation=layers.ReLU(max_value=6.0),\n                               use_bias=False,\n                               depthwise_regularizer=regularizers.l2(1e-5),\n                               pointwise_regularizer=regularizers.l2(1e-5))(x)\n    x = layers.UpSampling2D(2)(x)\n    x = layers.SeparableConv2D(64, 3, padding='same',\n                               activation=layers.ReLU(max_value=6.0),\n                               use_bias=False,\n                               depthwise_regularizer=regularizers.l2(1e-5),\n                               pointwise_regularizer=regularizers.l2(1e-5))(x)\n    x = layers.UpSampling2D(2)(x)\n    feat = layers.SeparableConv2D(64, 3, padding='same',\n                                  activation=layers.ReLU(max_value=6.0),\n                                  use_bias=False,\n                                  depthwise_regularizer=regularizers.l2(1e-5),\n                                  pointwise_regularizer=regularizers.l2(1e-5))(x)\n    heatmaps = layers.Conv2D(4, 1, padding='same', activation=None, use_bias=False, name='heatmaps')(feat)\n    coords = SoftArgmax(HEATMAP_SIZE, HEATMAP_SIZE, name='coords')(heatmaps)\n    return models.Model(inputs=inp, outputs=[heatmaps, coords])\n\nmodel = build_model()\n\ndef corner_error(y_true, y_pred):\n    err = tf.norm((y_true - y_pred) * ORIGINAL_DIM, axis=-1)\n    return tf.reduce_mean(err)\n\nmodel.compile(\n    optimizer=optimizers.Adam(1e-4),\n    loss={'heatmaps': 'mse', 'coords': 'mse'},\n    loss_weights={'heatmaps': 1.0, 'coords': 0.0},\n    metrics={'coords': corner_error}\n)\n\nmodel.fit(\n    train_ds,\n    validation_data=val_ds,\n    steps_per_epoch=len(train_anns) // BATCH_SIZE,\n    validation_steps=len(val_anns) // BATCH_SIZE,\n    epochs=EPOCHS,\n    callbacks=[\n        callbacks.EarlyStopping(\n            monitor='val_coords_corner_error',\n            mode='min',\n            patience=3,\n            restore_best_weights=True\n        )\n    ]\n)\n\n# Export only the coordinate output model for TF.js deployment\nmodel.export(MODEL_PATH)\nprint(\"âœ… WASM-compatible SavedModel exported to:\", MODEL_PATH)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.export(MODEL_PATH)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Convert to tensorflowjs for use on the web:","metadata":{}},{"cell_type":"code","source":"!pip install tensorflowjs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Convert the Keras model to tfjs graph model for use on the web:","metadata":{}},{"cell_type":"code","source":"#!/usr/bin/env python3\n\nimport os\nimport shutil\n\n# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSAVEDMODEL_DIR = \"/kaggle/working/mtg_keypoint_tfjs_safe_model\"  # No .h5 here!\nTFJS_OUT_DIR   = \"/kaggle/working/web_model\"\n\n# â”€â”€â”€ CLEAN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif not os.path.isdir(SAVEDMODEL_DIR):\n    raise FileNotFoundError(f\"âŒ SavedModel not found at: {SAVEDMODEL_DIR}\")\n\nshutil.rmtree(TFJS_OUT_DIR, ignore_errors=True)\nos.makedirs(TFJS_OUT_DIR, exist_ok=True)\n\n# â”€â”€â”€ CONVERT TO TFJS (with Float16 Quantization) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint(\"ðŸš€ Converting SavedModel to TensorFlow.js format with Float16 quantization...\")\nexit_code = os.system(\n    f\"tensorflowjs_converter \"\n    f\"--input_format=tf_saved_model \"\n    f\"--output_format=tfjs_graph_model \"\n    f\"--quantize_float16 \"\n    f\"--strip_debug_ops=true \"\n    f\"{SAVEDMODEL_DIR} \"\n    f\"{TFJS_OUT_DIR}\"\n)\n\nif exit_code != 0:\n    raise RuntimeError(f\"âŒ TFJS conversion failed with exit code {exit_code}\")\n\nprint(f\"âœ… Quantized TFJS GraphModel exported to: {TFJS_OUT_DIR}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport os\n\n# Paths\nweb_model_dir = os.path.join('/kaggle/working', 'web_model')\nzip_path = os.path.join('/kaggle/working', 'web_model')\n\n# Remove existing archive\nif os.path.exists(f\"{zip_path}.zip\"):\n    os.remove(f\"{zip_path}.zip\")\n\n# Create ZIP archive\nshutil.make_archive(zip_path, 'zip', web_model_dir)\n\nprint(f\"âœ… Created archive: {zip_path}.zip\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}